{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# word2vec\n",
    "\n",
    "> how do we make computers of today perform clustering, classification etc on a text data?\n",
    " \n",
    "**By creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- Word Embeddings are the texts converted into numbers\n",
    "- There may be different numerical representations of the same text\n",
    "- Formally, a Word Embedding format generally tries to map a word using a dictionary to a vector\n",
    "- A vector representation of a word may be a one-hot encoded vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of Word Embeddings\n",
    "\n",
    "- Frequency based Embedding\n",
    "  - Count Vector\n",
    "  - TF-IDF Vector\n",
    "  - Co-Occurrence Vector\n",
    "- Prediction based Embedding\n",
    "  - CBOW\n",
    "  - Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04164920/count-vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The matrix that will be prepared like above will be a very sparse one and inefficient for any computation. \n",
    "- So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorization\n",
    "\n",
    "- it takes into account not just the occurrence of a word in a single document but in the entire corpus\n",
    "- common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document.\n",
    "- Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.\n",
    "- TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF\n",
    "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04171138/Tf-IDF.png)\n",
    "- `TF(This,Document1)` = $\\frac{1}{8}$\n",
    "- `TF(This, Document2)`=$\\frac{1}{5}$\n",
    "- It denotes the contribution of the word to the document i.e words relevant to the document should be frequent.\n",
    "\n",
    "#### IDF\n",
    "- `IDF = log(N/n)`, where, N is the number of documents and n is the number of documents a term t has appeared in, N is the number of documents and n is the number of documents a term t has appeared in\n",
    "- IDF(This) = log(2/2) = 0\n",
    "- IDF(Messi) = log(2/1) = 0.301.\n",
    "- if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "- TF-IDF(This,Document1) = (1/8) * (0) = 0\n",
    "- TF-IDF(This, Document2) = (1/5) * (0) = 0\n",
    "- TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15\n",
    "- TF-IDF method heavily penalises the word ‘This’ but assigns greater weight to ‘Messi’. So, this may be understood as ‘Messi’ is an important word for Document1 from the context of the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Co-Occurrence Matrix with a fixed context window\n",
    "- **Similar words tend to occur together and will have similar context** – Apple is a fruit. Mango is a fruit.Apple and mango tend to have a similar context i.e fruit.\n",
    "- **Co-occurrence** – For a given corpus, the co-occurrence of a pair of words say $w_1$ and $w_2$ is the number of times they have appeared together in a Context Window.\n",
    "- **Context Window** – Context window is specified by a number and the direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction based Vector\n",
    "\n",
    "- Tomas Mikolov, 2013\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "- prediction based in the sense that they provided probabilities to the words\n",
    "- `King - man + woman = Queen`\n",
    "- a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model\n",
    "- shallow neural networks which map word(s) to the target variable which is also a word(s)\n",
    "- learn weights which act as word vector representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW (Continuous Bag of words)\n",
    "\n",
    "- predict the probability of a word given a context. \n",
    "  - A context may be a single word or a group of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, we have a corpus `C = “Hey, this is sample corpus using only one context word.”` and we have defined a context window of `1`. This corpus may be converted into a training set for a CBOW model as follow:\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04205949/cbow1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
