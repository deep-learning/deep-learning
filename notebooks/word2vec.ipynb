{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# word2vec\n",
    "\n",
    "> how do we make computers of today perform clustering, classification etc on a text data?\n",
    " \n",
    "**By creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- There may be different numerical representations of the same text\n",
    "- Formally, a Word Embedding format generally tries to map a word using a dictionary to a vector\n",
    "- A vector representation of a word may be a one-hot encoded vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04164920/count-vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The matrix that will be prepared like above will be a very sparse one and inefficient for any computation. \n",
    "- So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorization\n",
    "\n",
    "- it takes into account not just the occurrence of a word in a single document but in the entire corpus\n",
    "- common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document.\n",
    "- Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.\n",
    "- TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF\n",
    "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04171138/Tf-IDF.png)\n",
    "- `TF(This,Document1)` = $\\frac{1}{8}$\n",
    "- `TF(This, Document2)`=$\\frac{1}{5}$\n",
    "- It denotes the contribution of the word to the document i.e words relevant to the document should be frequent.\n",
    "\n",
    "#### IDF\n",
    "- `IDF = log(N/n)`, where, N is the number of documents and n is the number of documents a term t has appeared in, N is the number of documents and n is the number of documents a term t has appeared in\n",
    "- IDF(This) = log(2/2) = 0\n",
    "- IDF(Messi) = log(2/1) = 0.301.\n",
    "- if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "- TF-IDF(This,Document1) = (1/8) * (0) = 0\n",
    "- TF-IDF(This, Document2) = (1/5) * (0) = 0\n",
    "- TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15\n",
    "- TF-IDF method heavily penalises the word ‘This’ but assigns greater weight to ‘Messi’. So, this may be understood as ‘Messi’ is an important word for Document1 from the context of the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Co-Occurrence Matrix with a fixed context window\n",
    "- **Similar words tend to occur together and will have similar context** – Apple is a fruit. Mango is a fruit.Apple and mango tend to have a similar context i.e fruit.\n",
    "- **Co-occurrence** – For a given corpus, the co-occurrence of a pair of words say $w_1$ and $w_2$ is the number of times they have appeared together in a Context Window.\n",
    "- **Context Window** – Context window is specified by a number and the direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction based Vector\n",
    "\n",
    "- Tomas Mikolov, 2013\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "- prediction based in the sense that they provided probabilities to the words\n",
    "- `King - man + woman = Queen`\n",
    "- a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model\n",
    "- shallow neural networks which map word(s) to the target variable which is also a word(s)\n",
    "- learn weights which act as word vector representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, we have a corpus `C = “Hey, this is sample corpus using only one context word.”` and we have defined a context window of `1`. This corpus may be converted into a training set for a CBOW model as follow:\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04205949/cbow1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "\n",
    "- word embeddings or word Vectors are numerical representations of contextual similarities between words,\n",
    "- Finding the degree of similarity between two words: `model.similarity('woman','man')` => 0.737\n",
    "- Finding odd one out: `model.doesnt_match('breakfast cereal dinner lunch';.split())` => cereal\n",
    "- Amazing things like woman+king-man =queen: `model.most_similar(positive=['woman','king'],negative=['man'],topn=1)` => queen: 0.508\n",
    "- Probability of a text under the model: `model.score(['The fox jumped over the lazy dog'.split()])` => 0.21\n",
    "- It can be used to perform Machine Translation\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/05003807/ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Halfman Tree\n",
    "\n",
    "- frequence\n",
    "- encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierogical Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import pickle as pkl\n",
    "import re\n",
    "import jieba # chinese sentence splitting lib\n",
    "import os.path as path\n",
    "jieba.cut?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/zhenglai/data/stop_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-647b7fed4bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mraw_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-733f8812614c>\u001b[0m in \u001b[0;36mload_stop_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/zhenglai/data/stop_words.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/zhenglai/data/stop_words.txt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word frequency\n",
    "word_count = collections.Counter(raw_word_list)\n",
    "\n",
    "# retrain most common words\n",
    "word_count = word_count.most_common(30000)\n",
    "word_list = [x[0] for x in word_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a3c178b66303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m w2v = word2vec(vocab_list=word_list,\n\u001b[0m\u001b[1;32m     35\u001b[0m               \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_list' is not defined"
     ]
    }
   ],
   "source": [
    "class word2vec(object):\n",
    "    def __init__(self,\n",
    "                 vocab_list=None,\n",
    "                 embedding_size=200,\n",
    "                 win_len=3, # window length\n",
    "                 learning_rate=1,\n",
    "                 num_sampled=100):\n",
    "        self.batch_size = None\n",
    "        assert type(vocab_list) is list\n",
    "        self.vocab_list = vocab_list\n",
    "        self.learning_rate = learning_rate,\n",
    "        self.vocab_size = vocab_list._len_()\n",
    "        self.win_len = win_len\n",
    "        self.num_sampled = num_sampled\n",
    "        \n",
    "        self.wordid = {}\n",
    "        for i in range(self.vocab_size):\n",
    "            self.wordid[self.vocab_list[i]] = i\n",
    "        \n",
    "        self.train_words_num = 0\n",
    "        self.train_sentence_num = 0\n",
    "\n",
    "        self.build_graph()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.train_input = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "w2v = word2vec(vocab_list=word_list,\n",
    "              embedding_size=200,\n",
    "              learning_rate=1,\n",
    "              num_sampled=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: boto3 in /usr/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in /usr/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.9.0,>=1.8.29 in /usr/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/lib/python3.6/site-packages (from botocore<1.9.0,>=1.8.29->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/lib/python3.6/site-packages (from botocore<1.9.0,>=1.8.29->boto3->smart-open>=1.2.1->gensim)\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = ['the quick brown fox jumps over the lazy dogs', 'yoyoyo you go home now to sleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'],\n",
       " ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [s.split() for s in raw_sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-16 21:52:08,642 : INFO : collecting all words and their counts\n",
      "2018-01-16 21:52:08,642 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-16 21:52:08,643 : INFO : collected 15 word types from a corpus of 16 raw words and 2 sentences\n",
      "2018-01-16 21:52:08,643 : INFO : Loading a fresh vocabulary\n",
      "2018-01-16 21:52:08,643 : INFO : min_count=1 retains 15 unique words (100% of original 15, drops 0)\n",
      "2018-01-16 21:52:08,644 : INFO : min_count=1 leaves 16 word corpus (100% of original 16, drops 0)\n",
      "2018-01-16 21:52:08,644 : INFO : deleting the raw counts dictionary of 15 items\n",
      "2018-01-16 21:52:08,644 : INFO : sample=0.001 downsamples 15 most-common words\n",
      "2018-01-16 21:52:08,645 : INFO : downsampling leaves estimated 2 word corpus (13.7% of prior 16)\n",
      "2018-01-16 21:52:08,645 : INFO : estimated required memory for 15 words and 100 dimensions: 19500 bytes\n",
      "2018-01-16 21:52:08,645 : INFO : resetting layer weights\n",
      "2018-01-16 21:52:08,646 : INFO : training model with 3 workers on 15 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-16 21:52:08,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-16 21:52:08,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-16 21:52:08,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-16 21:52:08,648 : INFO : training on 80 raw words (12 effective words) took 0.0s, 11382 effective words/s\n",
      "2018-01-16 21:52:08,649 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `min_count`: control the frequency occurences, [0, 100]\n",
    "- `size`: dimensionality of the feature vector, large size == more inputs == more accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06415670792020713"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('dogs', 'you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034714531432736645"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('go', 'home')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
