{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "\n",
    "### Skip-Gram Model\n",
    "- Before skip-gram there was a bi-gram model which uses the most adjacent word to train the model. \n",
    "- But in this case the word can be any word inside the window. So you can use any of the words inside the window skipping the most adjacent word. Hence skip-gram.\n",
    "- We’re going to train a simple neural network with a single hidden layer to perform a certain task, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\n",
    "- unsupervised feature learning\n",
    "  - where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer--it's a trick for learning good image features without having labeled training data.\n",
    "- Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "- When I say \"nearby\", there is actually a \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\n",
    "- “The quick brown fox jumps over the lazy dog.” with windows size of 2\n",
    "![](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "- The network is going to learn the statistics from the number of times each pairing shows up\n",
    "\n",
    "#### Model details\n",
    "\n",
    "- we first build a vocabulary of words from our training documents, Let’s say we have a vocabulary of 10,000 unique words.\n",
    "- We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) \n",
    "- The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n",
    "- There is no activation function on the hidden layer neurons, but the output neurons use softmax.\n",
    "- When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution \n",
    "![](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "##### Hidden Layer\n",
    "\n",
    "- we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "- 300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a \"hyper parameter\" that you would just have to tune to your application (that is, try different values and see what yields the best results).\n",
    "[![](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n",
    "- what if multiply a 1 x 10,000 one-hot vector by a 10,0000 x 200 matrix, it's just operating as a lookup table.\n",
    "![](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    "\n",
    "##### Output layer\n",
    "- The output layer is a softmax regression classifier\n",
    "- each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function exp(x) to the result. Finally, in order to get the outputs to sum up to 1\n",
    "- neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after.\n",
    "- To understand the implication, let's say that in our training corpus, every single occurrence of the word 'York' is preceded by the word 'New'. That is, at least according to the training data, there is a 100% probability that 'New' will be in the vicinity of 'York'. However, if we take the 10 words in the vicinity of 'York' and randomly pick one of them, the probability of it being 'New' is not 100%; you may have picked one of the other words in the vicinity.\n",
    "![](http://mccormickml.com/assets/word2vec/output_weights_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intuition\n",
    "- If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is **if the word vectors are similar**. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!\n",
    "- for two words to have similar contexts\n",
    "  - synonyms like “intelligent” and “smart”\n",
    "  - fwords that are related, like “engine” and “transmission”\n",
    "  - handle stemming, like the network will likely learn similar word vectors for the words “ant” and “ants” \n",
    "- skip-gram neural network contains a huge number of weights\n",
    "  - with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each\n",
    "  - Training this on a large dataset would be prohibitive\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train?\n",
    "- Running gradient descent on a neural network that large is going to be slow.\n",
    "- And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting.\n",
    "- millions of weights times billions of training samples means that training this model is going to be a beast.\n",
    "\n",
    "#### Solutions\n",
    "- Treating common word pairs or phrases as single “words” in their model.\n",
    "- Subsampling frequent words to decrease the number of training examples.\n",
    "- Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.\n",
    "\n",
    "It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.\n",
    "\n",
    "\n",
    "##### Word Pairs and “Phrases”\n",
    "- a word pair like “Boston Globe” (a newspaper) has a much different meaning than the individual words “Boston” and “Globe”. So it makes sense to treat “Boston Globe”, wherever it occurs in the text, as a single word with its own word vector representation.\n",
    "- their published model, which was trained on 100 billion words from a Google News dataset.\n",
    "- The addition of phrases to the model swelled the vocabulary size to 3 million words!\n",
    "- Phrase detection\n",
    "  - Each pass of their tool only looks at combinations of 2 words, but you can run it multiple times to get longer phrases. So, the first pass will pick up the phrase “New_York”, and then running it again will pick up “New_York_City” as a combination of “New_York” and “City”.\n",
    "  - Each pass of their tool only looks at combinations of 2 words, but you can run it multiple times to get longer phrases. So, the first pass will pick up the phrase “New_York”, and then running it again will pick up “New_York_City” as a combination of “New_York” and “City”.\n",
    "  - The equation is designed to make phrases out of words which occur together often relative to the number of individual occurrences. \n",
    "  - an alternate phrase recognition strategy would be to use the titles of all Wikipedia articles as your vocabulary.\n",
    "  \n",
    "- Subsampling Frequent Words\n",
    "  - review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subsampling Frequent Words\n",
    "  - review training samples (word pairs) generation\n",
    "![](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "  - two “problems” with common words like “the”\n",
    "    - When looking at word pairs, (“fox”, “the”) doesn’t tell us much about the meaning of “fox”. “the” appears in the context of pretty much every word.\n",
    "    - We will have many more samples of (“the”, …) than we need to learn a good vector for “the”\n",
    "  - Word2Vec implements a “subsampling” scheme to address this. \n",
    "    - For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. \n",
    "    - The probability that we cut the word is related to the word’s frequency.\n",
    "    - If we have a window size of 10, and we remove a specific instance of “the” from our text:\n",
    "      - As we train on the remaining words, “the” will not appear in any of their context windows.\n",
    "      - We’ll have 10 fewer training samples where “the” is the input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling rate\n",
    "\n",
    "- $w_i$  is the word, $z(w_)$ is the fraction of the total words in the corpus that are that word.\n",
    "- $P(w_i)$ is the probability of keeping the word:\n",
    "\n",
    "![img_subsampling](files/word2vec_subsampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Sampling\n",
    "- each training sample will tweak all of the weights in the neural network\n",
    "- the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!\n",
    "- Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them\n",
    "- With negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word\n",
    "- The paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.\n",
    "- Recall that the output layer of our model has a weight matrix that’s 300 x 10,000. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!\n",
    "- In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
