{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/zhenglai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/zhenglai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already up-to-date: nltk in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already up-to-date: numpy in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already up-to-date: jieba in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already up-to-date: six in /usr/local/lib/python3.5/dist-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install -U nltk numpy jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3.2.5', '1.14.0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "nltk.__version__, np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy\n",
      "ibm1\n",
      "stanford\n",
      "crf\n",
      "texttiling\n",
      "inspect\n",
      "wordnet\n",
      "stanford_segmenter\n",
      "collocations\n",
      "ibm4\n",
      "metrics\n",
      "textcat\n",
      "corenlp\n",
      "chunk\n",
      "draw\n",
      "discourse\n",
      "rte_classify\n",
      "decorators\n",
      "lancaster\n",
      "hmm\n",
      "subprocess\n",
      "ibm2\n",
      "featurechart\n",
      "agreement\n",
      "help\n",
      "stack_decoder\n",
      "porter\n",
      "bllip\n",
      "locale\n",
      "senna\n",
      "api\n",
      "earleychart\n",
      "ibm3\n",
      "dependencygraph\n",
      "chart\n",
      "mace\n",
      "spearman\n",
      "sexpr\n",
      "data\n",
      "malt\n",
      "parse\n",
      "os\n",
      "classify\n",
      "ribes_score\n",
      "stem\n",
      "scores\n",
      "wsd\n",
      "boxer\n",
      "bisect\n",
      "regexp\n",
      "pydoc\n",
      "relextract\n",
      "viterbi\n",
      "transitionparser\n",
      "aline\n",
      "paice\n",
      "util\n",
      "tkinter\n",
      "segmentation\n",
      "shiftreduce\n",
      "distance\n",
      "downloader\n",
      "brill_trainer\n",
      "sys\n",
      "punkt\n",
      "logic\n",
      "ibm5\n",
      "tadm\n",
      "linearlogic\n",
      "misc\n",
      "simple\n",
      "mapping\n",
      "nonprojectivedependencyparser\n",
      "translate\n",
      "naivebayes\n",
      "mwe\n",
      "maxent\n",
      "snowball\n",
      "perceptron\n",
      "pchart\n",
      "positivenaivebayes\n",
      "treetransforms\n",
      "tbl\n",
      "repp\n",
      "brill\n",
      "re\n",
      "collections\n",
      "evaluate\n",
      "confusionmatrix\n",
      "ibm_model\n",
      "internals\n",
      "probability\n",
      "textwrap\n",
      "prover9\n",
      "scikitlearn\n",
      "tag\n",
      "tnt\n",
      "jsontags\n",
      "association\n",
      "treebank\n",
      "sequential\n",
      "inference\n",
      "featstruct\n",
      "lazyimport\n",
      "cluster\n",
      "weka\n",
      "decisiontree\n",
      "bleu_score\n",
      "types\n",
      "casual\n",
      "megam\n",
      "grammar\n",
      "tokenize\n",
      "ccg\n",
      "sem\n",
      "rslp\n",
      "recursivedescent\n",
      "tree\n",
      "text\n",
      "isri\n",
      "projectivedependencyparser\n",
      "hunpos\n",
      "compat\n",
      "lfg\n",
      "tableau\n",
      "glue\n",
      "toktok\n",
      "drt\n",
      "resolution\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\n",
    "for key, obj in nltk.__dict__.items():\n",
    "    if type(obj) is types.ModuleType: \n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/media/zhenglai/data/data/nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57340, 1161192)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents()), len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Allen', 'Jr.', '.', '``', 'Only', 'a', 'relative', 'handful', 'of', 'such']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '``',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'World',\n",
       " '!',\n",
       " \"''\",\n",
       " 'program',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'program',\n",
       " 'that',\n",
       " 'outputs',\n",
       " 'or',\n",
       " 'displays',\n",
       " '``',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'World',\n",
       " '!',\n",
       " \"''\",\n",
       " 'to',\n",
       " 'a',\n",
       " 'user',\n",
       " '.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"\"\"A \"Hello, World!\" program is a computer program that outputs or displays \"Hello, World!\" to a user.\"\"\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华/ 大/ 大学\n",
      "他/ 来到/ 了/ 网易/ 杭研大/ 大厦\n",
      "小明/ 硕士/ 士/ 毕业/ 于/ 中国/ 科学/ 学院/ 科学院/ 中国科学院/ 计算/ 计算所/ ,/ 后/ 在/ 日本/ 京都/ 大学/ 日本京都大学/ 深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode:\", \"/ \".join(seg_list)) # 全模式\n",
    "seg_list = jieba.cut(\"我来到北京清华大大学\", cut_all=False)\n",
    "print(\"Default Mode:\", \"/ \".join(seg_list)) # 精确模式\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大大厦\") # 默认是精确模式\n",
    "print(\"/ \".join(seg_list))\n",
    "seg_list = jieba.cut_for_search(\"小明硕士士毕业于中国科学院计算所,后在日本京都大学深造\")\n",
    "# 搜索引擎模式\n",
    "print(\"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天天气 /真不错'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" /\".join(jieba.cut('今天天气真不错', cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what /a /nice /weather /today'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" /\".join(nltk.word_tokenize('what a nice weather today'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'somebody', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tweet = 'RT @somebody: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@somebody', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "emoticons_str = r\"\"\"\n",
    "(?:\n",
    "[:=;] # 眼睛\n",
    "[oO\\-]? # 鼻鼻子子\n",
    "[D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    ")\"\"\"\n",
    "regex_str = [\n",
    "emoticons_str,\n",
    "r'<[^>]+>', # HTML tags\n",
    "r'(?:@[\\w_]+)', # @某人人\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    "# URLs\n",
    "r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词\n",
    "r'(?:[\\w_]+)', # 其他\n",
    "r'(?:\\S)' # 其他\n",
    "]\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.ssearch(token) else token.lower() for token in\n",
    "        tokens]\n",
    "    return tokens\n",
    "tweet = 'RT @somebody: love you baby! :D http://ah.love #168cm'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 纷繁复杂的词形\n",
    "- inflection变化: walk => walking => walked: 不影响词性\n",
    "- derivation 引申: nation (noun) => national (adjective) => nationalize (verb):影响词性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词形归一化\n",
    "- Stemming 词干提取:一般来说,就是把不影响词性的inflection的小尾巴砍掉\n",
    "    - 基于规则\n",
    "    - walking 砍ing = walk\n",
    "    - walked 砍ed = walk\n",
    "- Lemmatization 词形归一:把各种类型的词的变形,都归为一个形式\n",
    "    - 基于字典\n",
    "    - went 归一 = go\n",
    "      - Went n. 英文名:温特\n",
    "    - are 归一 = be\n",
    "\n",
    "\n",
    ">refs: http://blog.csdn.net/march_on/article/details/8935462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolves      ->  wolv\n",
      "maximum     ->  maximum\n",
      "presumably  ->  presum\n",
      "went        ->  went\n",
      "wenting     ->  went\n",
      "walking     ->  walk\n",
      "walked      ->  walk\n",
      "abaci       ->  abaci\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "for w in ['wolves', 'maximum', 'presumably', 'went', 'wenting', 'walking', 'walked', 'abaci']:\n",
    "    print('{}  ->  {}'.format(w.ljust(10), porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 110]\n",
      "[nltk_data]     Connection timed out>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/home/zhenglai/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/home/zhenglai/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6efa978a1148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'wolves'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maximum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'presumably'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'went'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wenting'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'walking'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'walked'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abaci'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}  ->  {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/home/zhenglai/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in ['wolves', 'maximum', 'presumably', 'went', 'wenting', 'walking', 'walked', 'abaci']:\n",
    "    print('{}  ->  {}'.format(w.ljust(10), lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS\n",
    "\n",
    "- Part-of-Speach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n"
     ]
    }
   ],
   "source": [
    "for n in ['JJ', 'IN', 'NNP', 'NN', 'V', 'NNS', 'WDT']:\n",
    "    nltk.help.upenn_tagset(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('are', 'is')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default to `NN`\n",
    "lemmatizer.lemmatize('are'), lemmatizer.lemmatize('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('be', 'be')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add POS tag\n",
    "lemmatizer.lemmatize('are', pos='v'), lemmatizer.lemmatize('is', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['what', 'does', 'the', 'fox', 'say'],\n",
       " [('what', 'WDT'),\n",
       "  ('does', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('fox', 'NNS'),\n",
       "  ('say', 'VBP')])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize('what does the fox say')\n",
    "text, nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "- 一千个HE有一千种指代\n",
    "- 一千个THE有一千种指事\n",
    "- 对于注重理解文本『意思』的应用场景来说, 歧义太多\n",
    "- http://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zhenglai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I',\n",
       "  'am',\n",
       "  'ok',\n",
       "  ',',\n",
       "  'you',\n",
       "  'are',\n",
       "  'fine',\n",
       "  ',',\n",
       "  'and',\n",
       "  'he',\n",
       "  'is',\n",
       "  'good',\n",
       "  'now'],\n",
       " ['I', 'ok', ',', 'fine', ',', 'good'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "word_list = nltk.word_tokenize('I am ok, you are fine, and he is good now')\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "word_list, filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, display; \n",
    "Image(filename='../files/00_word2vec_00_text_preprocessing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情感分析\n",
    "\n",
    "- 最简单的 sentiment dictionary\n",
    "  - like 1\n",
    "  - good 2\n",
    "  - bad -2\n",
    "  - terrible -3\n",
    "  - 类似于关键词打分机制\n",
    "  - [AFINN-111](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)\n",
    "  - 太Naive\n",
    "  - 新词怎么办?\n",
    "  - 特殊词汇怎么办"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dictionary = {}\n",
    "for line in open('/media/zhenglai/data/data/AFINN/AFINN-111.txt'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)\n",
    "# 把这个打分表记录在一一个Dict上以后\n",
    "# 跑一一遍整个句句子子,把对应的值相加\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in word_list)\n",
    "# 有值就是Dict中的值,没有就是0\n",
    "# 于是你就得到了了一一个 sentiment score\n",
    "total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buoyant', 2),\n",
       " ('terror', -3),\n",
       " ('ominous', 3),\n",
       " ('bribe', -3),\n",
       " ('disastrous', -3),\n",
       " ('please', 1),\n",
       " ('affronted', -1),\n",
       " ('strong', 2),\n",
       " ('slut', -5),\n",
       " ('pathetic', -2)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sentiment_dictionary.items())[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配上ML的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "pos\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "# 随手手造点训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "def preprocess(s):\n",
    "    return {word: True for word in s.lower().split()}\n",
    "\n",
    "# 把训练集给做成标准形式\n",
    "training_data = [\n",
    "    [preprocess(s1), 'pos'],\n",
    "    [preprocess(s2), 'pos'],\n",
    "    [preprocess(s3), 'neg'],\n",
    "    [preprocess(s4), 'neg']]\n",
    "# 喂给model吃\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "# 打出结果\n",
    "print(model.classify(preprocess('this is not a good book')))\n",
    "print(model.classify(preprocess('this is a good person')))\n",
    "print(model.classify(preprocess('this is a bad person')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF: Term Frequency, 衡量一个term在文档中出现得有多频繁。\n",
    "TF(t) = (t出现在文档中的次数) / (文档中的term总数).\n",
    "- IDF: Inverse Document Frequency, 衡量一个term有多重要。\n",
    "有些词出现的很多,但是明显不是很有卵用。比如’is',’the‘,’and‘之类\n",
    "的。为了平衡,我们把罕见的词的重要性(weight)搞高,\n",
    "把常见词的重要性搞低。\n",
    "  - IDF(t) = log_e(文档总数 / 含有t的文档总数).\n",
    "- TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一个文档有100个单词,其中单词baby出现了3次。那么,TF(baby) = (3/100) = 0.03.\n",
    "- 现在我们如果有10M的文档, baby出现在其中的1000个文档中。\n",
    "那么,IDF(baby) = log(10,000,000 / 1,000) = 4\n",
    "- 所以, TF-IDF(baby) = TF(baby) x IDF(baby) = 0.03 * 4 = 0.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "- 传统: 基于规则\n",
    "- 现代:基于统计机器学习\n",
    "  - HMM, CRF, SVM, LDA, CNN...\n",
    "  - “规则”隐含在模型参数里\n",
    "  \n",
    "## Word Embedding\n",
    "- Word Embeddings are the texts converted into numbers\n",
    "- 词编码需要保证词的相似性\n",
    "  - nearest words to frog? toad, litoria, rana, lizard...\n",
    "- 简单 词/短语 翻译\n",
    "  - 向量空间分布的相似性\n",
    "- 向量空间子结构\n",
    "  - $$V_{King} - V_{Queen} + V_{Women} = V_{Man}$$\n",
    "  - $$V_{Paris} - V_{France} + V_{German} = V_{Berlin}$$\n",
    "- 词向量表示作为机器学习、特别是深度学习的输入和表示空间\n",
    "- 离散表示: One-hot表示\n",
    "  - 词典包含10个单词,每个单词有唯一索引\n",
    "  - 在词典中的顺序和在句子中的顺序没有关联"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of Word Embeddings\n",
    "\n",
    "- Frequency based Embedding\n",
    "  - Count Vector\n",
    "  - TF-IDF Vector\n",
    "  - Co-Occurrence Vector\n",
    "- Prediction based Embedding\n",
    "  - words that appear in the same contexts share semantic meaning\n",
    "    - CBOW\n",
    "    - Skip-Gram\n",
    "  - good properties\n",
    "    - word embeddings or word Vectors are numerical representations of contextual similarities between words,\n",
    "    - Finding the degree of similarity between two words: `model.similarity('woman','man')` => 0.737\n",
    "    - Finding odd one out: `model.doesnt_match('breakfast cereal dinner lunch';.split())` => cereal\n",
    "    - Amazing things like woman+king-man =queen: `model.most_similar(positive=['woman','king'],negative=['man'],topn=1)` => queen: 0.508\n",
    "    - Probability of a text under the model: `model.score(['The fox jumped over the lazy dog'.split()])` => 0.21\n",
    "    - It can be used to perform Machine Translation\n",
    "    ![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/05003807/ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured*\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/05003425/graph1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload-images.jianshu.io/upload_images/712028-2b38e09f94542c64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW (Continuous Bag of words)\n",
    "\n",
    "- predict the probability of a word given a context. \n",
    "  - A context may be a single word or a group of words\n",
    "  - CBOW在许多分布式信息上进行平滑（将整个上下文作为一种情况），大多数情况下这个模型在小一点的数据集上更有效"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Model\n",
    "- skip-gram将每一对context-target词作为一种新的情况，在大一点的数据集上的会有更好效果\n",
    "- Before skip-gram there was a bi-gram model which uses the most adjacent word to train the model. \n",
    "- But in this case the word can be any word inside the window. So you can use any of the words inside the window skipping the most adjacent word. Hence skip-gram.\n",
    "- unsupervised feature learning\n",
    "  - where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer\n",
    "  - it's a trick for learning good image features without having labeled training data.\n",
    "- window size\n",
    "  - A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\n",
    "- “The quick brown fox jumps over the lazy dog.” with windows size of 2\n",
    "![](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "- The network is going to learn the statistics from the number of times each pairing shows up\n",
    "\n",
    "#### Model details\n",
    "\n",
    "- we first build a vocabulary of words from our training documents, Let’s say we have a vocabulary of 10,000 unique words.\n",
    "- Input as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) \n",
    "- The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n",
    "- There is no activation function on the hidden layer neurons, but the output neurons use softmax.\n",
    "![](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "##### Hidden Layer\n",
    "\n",
    "-  A weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "- 300 features is what Google used in their published model trained on the Google news dataset - The number of features is a \"hyper parameter\".\n",
    "[![](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n",
    "- what if multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it's just operating as a lookup table.\n",
    "![](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    "\n",
    "##### Output layer\n",
    "- The output layer is a softmax regression classifier\n",
    "- neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after\n",
    "![](http://mccormickml.com/assets/word2vec/output_weights_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intuition\n",
    "- If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. - And one way for the network to output similar context predictions for these two words is **if the word vectors are similar**. \n",
    "- If two words have similar contexts, then our network is motivated to learn similar word vectors for these two words!\n",
    "- for two words to have similar contexts\n",
    "  - synonyms like “intelligent” and “smart”\n",
    "  - related, like “engine” and “transmission”\n",
    "  - easier stemming, like the network will likely learn similar word vectors for the words “ant” and “ants” \n",
    "- skip-gram neural network contains a huge number of weights\n",
    "  - with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each\n",
    "  - Training this on a large dataset would be prohibitive\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "- Need huge amount of training data in order to tune that many weights and avoid over-fitting\n",
    "- Slow\n",
    "\n",
    "#### Solutions\n",
    "- Treating common word pairs or phrases as single “words” in their model.\n",
    "- Subsampling frequent words to decrease the number of training examples.\n",
    "- Modifying the optimization objective with a technique they called “Negative Sampling”, which causes each training sample to update only a small percentage of the model’s weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NCE\n",
    "- 对于每一个样本，除了他自己的label，同时采样出N个其他的label，从而我们只需要计算样本在这N+1个label上的概率，而不用计算样本在所有label上的概率.\n",
    "![](https://upload-images.jianshu.io/upload_images/5843610-1e9192a53aa78286.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/586)\n",
    "- softmax的分母要计算整个词表的得分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google pre-trained Word2Vec model\n",
    "- It’s 1.5GB! \n",
    "- It includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. \n",
    "- The vector length is 300 features.\n",
    "\n",
    "###### Interesting aspects\n",
    "\n",
    "- Does it include stop words?\n",
    "Answer: Some stop words like \"a\", \"and\", \"of\" are excluded, but others like \"the\", \"also\", \"should\" are included.\n",
    "- Does it include misspellings of words?\n",
    "Answer: Yes. For instance, it includes both \"mispelled\" and \"misspelled\"--the latter is the correct one.\n",
    "- Does it include commonly paired words?\n",
    "Answer: Yes. For instance, it includes \"Soviet_Union\" and \"New_York\".\n",
    "- Does it include numbers?\n",
    "Answer: Not directly; e.g., you won't find \"100\". But it does include entries like \"###MHz_DDR2_SDRAM\" where I'm assuming the '#' are intended to match any digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-26 15:35:23,358 : INFO : loading projection weights from /media/zhenglai/data/model/GoogleNews-vectors-negative300.bin\n",
      "2018-01-26 15:36:01,638 : INFO : loaded (3000000, 300) matrix from /media/zhenglai/data/model/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/media/zhenglai/data/model/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "# memory cost: 3 million words * 300 features * 4bytes/feature = ~3.35GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('a' in model.vocab, 'and' in model.vocab, 'the' in model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\" in model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"New_York\" in model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189672946929932),\n",
       " ('princess', 0.5902429819107056),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236843824386597),\n",
       " ('Queen_Consort', 0.5235944986343384),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098592638969421),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7291509509086609),\n",
       " ('bad', 0.7190051078796387),\n",
       " ('terrific', 0.6889116168022156),\n",
       " ('decent', 0.6837347149848938),\n",
       " ('nice', 0.683609127998352),\n",
       " ('excellent', 0.6442930102348328),\n",
       " ('fantastic', 0.6407779455184937),\n",
       " ('better', 0.6120728850364685),\n",
       " ('solid', 0.5806034803390503),\n",
       " ('lousy', 0.5764201879501343)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NaiveBayesClassifier' object has no attribute 'most_similar_cosmul'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-7657b59dc3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar_cosmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NaiveBayesClassifier' object has no attribute 'most_similar_cosmul'"
     ]
    }
   ],
   "source": [
    "model.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.9314123392105103),\n",
       " ('monarch', 0.858533501625061),\n",
       " ('princess', 0.8476566076278687),\n",
       " ('Queen_Consort', 0.8150269389152527),\n",
       " ('queens', 0.8099815845489502),\n",
       " ('crown_prince', 0.808997631072998),\n",
       " ('royal_palace', 0.8027306795120239),\n",
       " ('monarchy', 0.801961362361908),\n",
       " ('prince', 0.8009798526763916),\n",
       " ('empress', 0.7958388328552246)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 16:32:57,975 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"apple google microsoft facebook\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orange'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"apple google microsoft facebook　orange\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7664012230995353"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8543271914204855"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('boy', 'girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(w, wl):\n",
    "    return [model.similarity(w, x) for x in wl] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24305128010006555,\n",
       " 0.2277479958585296,\n",
       " 0.31990406601708554,\n",
       " 0.35862676003165994]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('apple', ['mac', 'google', 'iphone', 'ipad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12329282143219919,\n",
       " 0.0545070610444016,\n",
       " 0.15719912945159908,\n",
       " 0.1411240004159244]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('orange', ['mac', 'google', 'iphone', 'ipad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.029601076652710112,\n",
       " -0.002686200359970803,\n",
       " -0.0627914307547104,\n",
       " -0.1003647502897477]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('deep', ['mac', 'google', 'iphone', 'ipad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.028935402538443355,\n",
       " 0.06100333246535389,\n",
       " 0.01974472197122787,\n",
       " 0.02710275658467912]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity('test', ['mac', 'google', 'iphone', 'ipad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 4, 7, 3, 7, 4, 2]\n",
      "[[7], [6], [1], [3], [6], [7], [0], [3]]\n",
      "['six', 'four', 'one', 'six', 'four', 'six', 'one', 'eight']\n",
      "['six', 'two', 'seven', 'four', 'two', 'six', 'five', 'four']\n",
      "Loss at 0: 7.35702\n",
      "Loss at 100: 3.44722\n",
      "Loss at 200: 2.91450\n",
      "Loss at 300: 2.71819\n",
      "Loss at 400: 2.60483\n",
      "Loss at 500: 2.59928\n",
      "Loss at 600: 2.54176\n",
      "Loss at 700: 2.55187\n",
      "Loss at 800: 2.52209\n",
      "Loss at 900: 2.48728\n",
      "testing cosine dists to one\n",
      "seven -> 0.8460716\n",
      "three -> 0.8399113\n",
      "nine -> 0.73751897\n",
      "five -> 0.64920115\n",
      "two -> -0.20698225\n",
      "eight -> -0.21769652\n",
      "six -> -0.24484032\n",
      "four -> -0.26775\n",
      "\n",
      "testing cosine dists to two\n",
      "six -> 0.8733612\n",
      "four -> 0.8454992\n",
      "eight -> 0.5339997\n",
      "seven -> 0.069704965\n",
      "nine -> -0.13519338\n",
      "one -> -0.20698225\n",
      "three -> -0.25314385\n",
      "five -> -0.4006742\n",
      "\n",
      "testing cosine dists to three\n",
      "nine -> 0.9626806\n",
      "one -> 0.8399113\n",
      "seven -> 0.71625143\n",
      "five -> 0.68355066\n",
      "eight -> -0.19226906\n",
      "two -> -0.25314385\n",
      "four -> -0.31192645\n",
      "six -> -0.33846265\n",
      "\n",
      "testing cosine dists to four\n",
      "six -> 0.98992825\n",
      "eight -> 0.8632992\n",
      "two -> 0.8454992\n",
      "five -> -0.15631771\n",
      "seven -> -0.26659134\n",
      "one -> -0.26775\n",
      "nine -> -0.3099615\n",
      "three -> -0.31192645\n",
      "\n",
      "testing cosine dists to five\n",
      "three -> 0.68355066\n",
      "one -> 0.64920115\n",
      "nine -> 0.48473856\n",
      "seven -> 0.3226171\n",
      "eight -> -0.061262086\n",
      "six -> -0.15534855\n",
      "four -> -0.15631771\n",
      "two -> -0.4006742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "batch_size = 64\n",
    "embedding_dimension = 5\n",
    "negative_samples = 8\n",
    "LOG_DIR = \"logs/word2vec_test\"\n",
    "\n",
    "digit_to_word_map = {1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\",\n",
    "                     6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\"}\n",
    "sentences = []\n",
    "\n",
    "# Create two kinds of sentences - sequences of odd and even digits.\n",
    "for i in range(10000):\n",
    "    rand_odd_ints = np.random.choice(range(1, 10, 2), 3)\n",
    "    sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    rand_even_ints = np.random.choice(range(2, 10, 2), 3)\n",
    "    sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "\n",
    "# Map words to indices\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "# Generate skip-gram pairs\n",
    "skip_gram_pairs = []\n",
    "for sent in sentences:\n",
    "    tokenized_sent = sent.lower().split()\n",
    "    for i in range(1, len(tokenized_sent) - 1):\n",
    "        # [[before, after], current]\n",
    "        word_context_pair = [[word2index_map[tokenized_sent[i - 1]],\n",
    "                              word2index_map[tokenized_sent[i + 1]]],\n",
    "                             word2index_map[tokenized_sent[i]]]\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                                word_context_pair[0][0]])\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                                word_context_pair[0][1]])\n",
    "\n",
    "\n",
    "def get_skipgram_batch(batch_size):\n",
    "    instance_indices = list(range(len(skip_gram_pairs)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch]\n",
    "    y = [[skip_gram_pairs[i][1]] for i in batch]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# batch example\n",
    "x_batch, y_batch = get_skipgram_batch(8)\n",
    "print(x_batch)\n",
    "print(y_batch)\n",
    "print([index2word_map[word] for word in x_batch])\n",
    "print([index2word_map[word[0]] for word in y_batch])\n",
    "\n",
    "# Input data, labels\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# Embedding lookup table currently only implemented in CPU\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_dimension],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    # a lookup table\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# Create variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_dimension],\n",
    "                        stddev=1.0 / math.sqrt(embedding_dimension)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   inputs=embed,\n",
    "                   labels=train_labels,\n",
    "                   num_sampled=negative_samples,\n",
    "                   num_classes=vocabulary_size))\n",
    "tf.summary.scalar(\"NCE_loss\", loss)\n",
    "\n",
    "# learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.1,\n",
    "                                          global_step=global_step,\n",
    "                                          decay_steps=1000,\n",
    "                                          decay_rate=0.95,\n",
    "                                          staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR, graph=tf.get_default_graph())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with open(os.path.join(LOG_DIR, 'metadata.tsv'), \"w\") as metadata:\n",
    "        metadata.write('Name\\tClass\\n')\n",
    "        for k, v in index2word_map.items():\n",
    "            metadata.write('%s\\t%d\\n' % (v, k))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embeddings.name\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch = get_skipgram_batch(batch_size)\n",
    "        summary, _ = sess.run([merged, train_step],\n",
    "                              feed_dict={train_inputs: x_batch,\n",
    "                                         train_labels: y_batch})\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, os.path.join(LOG_DIR, \"w2v_model.ckpt\"), step)\n",
    "            loss_value = sess.run(loss,\n",
    "                                  feed_dict={train_inputs: x_batch,\n",
    "                                             train_labels: y_batch})\n",
    "            print(\"Loss at %d: %.5f\" % (step, loss_value))\n",
    "\n",
    "    # Normalize embeddings before using\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    normalized_embeddings_matrix = sess.run(normalized_embeddings)\n",
    "\n",
    "def test_word(word_str):\n",
    "    ref_word = normalized_embeddings_matrix[word2index_map[word_str]]\n",
    "\n",
    "    cosine_dists = np.dot(normalized_embeddings_matrix, ref_word)\n",
    "    ff = np.argsort(cosine_dists)[::-1][1:10]\n",
    "    print(\"testing cosine dists to {}\".format(word_str))\n",
    "    for f in ff:\n",
    "        print(index2word_map[f], '->', cosine_dists[f])\n",
    "    print()\n",
    "\n",
    "test_word('one')\n",
    "test_word('two')\n",
    "test_word('three')\n",
    "test_word('four')\n",
    "test_word('five')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
