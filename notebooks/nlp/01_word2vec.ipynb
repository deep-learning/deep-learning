{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "\n",
    "### Skip-Gram Model\n",
    "- We’re going to train a simple neural network with a single hidden layer to perform a certain task, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\n",
    "- unsupervised feature learning\n",
    "  - where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer--it's a trick for learning good image features without having labeled training data.\n",
    "- Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "- When I say \"nearby\", there is actually a \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\n",
    "- “The quick brown fox jumps over the lazy dog.” with windows size of 2\n",
    "![](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "- The network is going to learn the statistics from the number of times each pairing shows up\n",
    "\n",
    "#### Model details\n",
    "\n",
    "- we first build a vocabulary of words from our training documents, Let’s say we have a vocabulary of 10,000 unique words.\n",
    "- We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) \n",
    "- The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n",
    "- There is no activation function on the hidden layer neurons, but the output neurons use softmax.\n",
    "- When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution \n",
    "![](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "##### Hidden Layer\n",
    "\n",
    "- we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "- 300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a \"hyper parameter\" that you would just have to tune to your application (that is, try different values and see what yields the best results).\n",
    "[![](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
